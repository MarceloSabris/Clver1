{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f927a374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fim\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd   \n",
    "import os\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import json\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import os\n",
    "import nltk\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "import pickle\n",
    "print('fim')\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c103e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:4: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "from keras.backend import manual_variable_initialization\n",
    "manual_variable_initialization(True)\n",
    "start_time = time.clock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8fa14276",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GravarArquivo ( data_dict,fname):\n",
    "    fname = fname +\"_\" +str(len(data_dict)) + '.json'\n",
    "    print(\"gravar arquivo: \" + fname + \" qtd: \" +  str(len(data_dict)))\n",
    "    os.makedirs('Percentage_'+ str(percentageData) , exist_ok=True)\n",
    "    fname = 'Percentage_'+ str(percentageData) + \"/\" + fname\n",
    "    # Create file\n",
    "    with open(fname, 'w') as outfile:\n",
    "        json.dump(data_dict, outfile, ensure_ascii=False, indent=4) \n",
    "        outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aaf6837d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(history):\n",
    "            plt.plot(history.history['loss'], label='training loss')\n",
    "            plt.plot(history.history['val_loss'], label=' validation loss')\n",
    "            #plt.ylim([0, 10])\n",
    "            plt.xlabel('Epopch')\n",
    "            plt.ylabel('Error')\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.savefig( 'Percentage_'+ str(percentageData) + '/error_datalenght.png')\n",
    "            plt.show()\n",
    "\n",
    "def plot_acc(history):  \n",
    "            plt.plot(history.history['sparse_categorical_accuracy'])\n",
    "            plt.plot(history.history['val_sparse_categorical_accuracy'])\n",
    "            plt.title('model accuracy')\n",
    "            plt.ylabel('accuracy')\n",
    "            plt.xlabel('epoch')\n",
    "            plt.legend(['train', 'val'], loc='upper left')\n",
    "            plt.grid(True)\n",
    "            plt.savefig('Percentage_'+ str(percentageData) + '/accuracy_datalenght.png')\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "03953330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs : 0\n",
      "Tensorflow GPU : True\n"
     ]
    }
   ],
   "source": [
    "#Check GPU is available for training or not Or whether the tensorflow version can utilize gpu \n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "print(\"Number of GPUs :\", len(physical_devices)) \n",
    "print(\"Tensorflow GPU :\",tf.test.is_built_with_cuda())\n",
    "if len(physical_devices)>0:\n",
    "    device=\"/GPU:0\"\n",
    "else:\n",
    "    device=\"/CPU:0\"\n",
    "percentageData = 3\n",
    "posTrainList=[]\n",
    "posValList=[]\n",
    "BATCH_SIZE=100\n",
    "IMG_SIZE=(200,200)\n",
    "QtdEpocasGravarCHKP = 1\n",
    "Epochs = 40\n",
    "lenghtDataTrain = 0\n",
    "lenghtDataVal =0 \n",
    "weight = 'weight-050'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da49cf56",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open( 'Percentage_'+str(percentageData) +  '/train_dataframe', \"rb\") as f:\n",
    "     train_dataframe =  pickle.load(f) \n",
    "with open( 'Percentage_'+str(percentageData) +  '/encoder', \"rb\") as f:\n",
    "     encoder =  pickle.load(f)\n",
    "with open( 'Percentage_'+str(percentageData) +  '/val_dataframe', \"rb\") as f:\n",
    "     val_dataframe =  pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "77854753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"EagerPyFunc_1:0\", shape=(1,), dtype=int32, device=/job:localhost/replica:0/task:0)\n",
      "Tensor(\"EagerPyFunc_1:0\", shape=(1,), dtype=int32, device=/job:localhost/replica:0/task:0)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Function that uses the encoder created to encode the input question and answer string\n",
    "def encode_fn(text):\n",
    "    return np.array(encoder.encode(text.numpy()))\n",
    "\n",
    "\n",
    "#Function to load and decode the image from the file paths in the dataframe and use the encoder function\n",
    "def preprocess(ip,ans):\n",
    "    img,ques=ip#ip is a list containing image paths and questions\n",
    "    img=tf.io.read_file(img)\n",
    "    img=tf.image.decode_jpeg(img,channels=3)\n",
    "    # quantos canais de cores tem \n",
    "    img=tf.image.resize(img,IMG_SIZE)\n",
    "    img=tf.math.divide(img, 255)# \n",
    "    #The question string is converted to encoded list with fixed size of 50 with padding with 0 value\n",
    "    ques=tf.py_function(encode_fn,inp=[ques],Tout=tf.int32)\n",
    "    paddings = [[0, 50-tf.shape(ques)[0]]]\n",
    "    ques = tf.pad(ques, paddings, 'CONSTANT', constant_values=0)\n",
    "    ques.set_shape([50])#Explicit shape must be defined in order to create the Input pipeline\n",
    "    \n",
    "    #The Answer is also encoded \n",
    "    ans=tf.py_function(encode_fn,inp=[ans],Tout=tf.int32)\n",
    "    ans.set_shape([1])\n",
    "    print(ans)\n",
    "    return (img,ques),ans\n",
    "    \n",
    "def create_pipeline(dataframe):\n",
    "    raw_df=tf.data.Dataset.from_tensor_slices(((dataframe['Path'],dataframe['Question']),dataframe['Answer']))\n",
    "    df=raw_df.map(preprocess)#Preprocessing function is applied to the dataset\n",
    "    df=df.batch(BATCH_SIZE)#The dataset is batched\n",
    "    return df\n",
    "\n",
    "#The training and validation Dataset objects are created\n",
    "train_dataset=create_pipeline(train_dataframe)\n",
    "validation_dataset=create_pipeline(val_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fcd8f928",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "Unsuccessful TensorSliceReader constructor: Failed to find any matching files for Percentage_3/weight-050",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/py_checkpoint_reader.py\u001b[0m in \u001b[0;36mNewCheckpointReader\u001b[0;34m(filepattern)\u001b[0m\n\u001b[1;32m     94\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mCheckpointReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m   \u001b[0;31m# TODO(b/143319754): Remove the RuntimeError casting logic once we resolve the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for Percentage_3/weight-050",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_28752/1613297085.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Percentage_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpercentageData\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;34m+\u001b[0m\u001b[0;34m\"/ModelTreinamento\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mweight\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Percentage_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpercentageData\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\"/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mweight\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch, options)\u001b[0m\n\u001b[1;32m   2197\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2198\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2199\u001b[0;31m         \u001b[0mpy_checkpoint_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNewCheckpointReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2200\u001b[0m         \u001b[0msave_format\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'tf'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2201\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0merrors_impl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLossError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/py_checkpoint_reader.py\u001b[0m in \u001b[0;36mNewCheckpointReader\u001b[0;34m(filepattern)\u001b[0m\n\u001b[1;32m     97\u001b[0m   \u001b[0;31m# issue with throwing python exceptions from C++.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m     \u001b[0merror_translator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/py_checkpoint_reader.py\u001b[0m in \u001b[0;36merror_translator\u001b[0;34m(e)\u001b[0m\n\u001b[1;32m     33\u001b[0m       \u001b[0;34m'Failed to find any '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m       'matching files for') in error_message:\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0merrors_impl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m   elif 'Sliced checkpoints are not supported' in error_message or (\n\u001b[1;32m     37\u001b[0m       \u001b[0;34m'Data type '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for Percentage_3/weight-050"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model('Percentage_'+str(percentageData)  +\"/ModelTreinamento\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "41fd018c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "text_input (InputLayer)         [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 50, 256)      26112       text_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "image_input (InputLayer)        [(None, 200, 200, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   (None, 50, 512)      1050624     embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mobilenetv2_1.00_224 (Functiona (None, 7, 7, 1280)   2257984     image_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 50, 512)      1574912     bidirectional[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d (Globa (None, 1280)         0           mobilenetv2_1.00_224[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 1024)         4198400     bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 2304)         0           global_average_pooling2d[0][0]   \n",
      "                                                                 bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 102)          235110      concatenate[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 9,343,142\n",
      "Trainable params: 9,309,030\n",
      "Non-trainable params: 34,112\n",
      "__________________________________________________________________________________________________\n",
      "0\n",
      "1\n",
      "20999\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "if weight != \"\":\n",
    "    model.load_weights('Percentage_'+str(percentageData) +\"/\" + weight+'.ckpt' )\n",
    "    model.summary()\n",
    "\n",
    "print(QtdEpocasGravarCHKP*lenghtDataTrain)\n",
    "lenghtDataTrain = len(train_dataframe)\n",
    "print(QtdEpocasGravarCHKP)\n",
    "print(lenghtDataTrain)\n",
    "print(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3c1a63cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4499\n",
      "20999\n"
     ]
    }
   ],
   "source": [
    "print (len(val_dataframe))\n",
    "print (len(train_dataframe))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ade90b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20999\n",
      "Epoch 1/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-06 00:49:49.245303: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-06 00:49:49.246518: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2199995000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1050/1050 [==============================] - 1693s 2s/step - loss: 0.4478 - sparse_categorical_accuracy: 0.8208 - val_loss: 6.5568 - val_sparse_categorical_accuracy: 0.3350\n",
      "Epoch 2/40\n",
      "1050/1050 [==============================] - 1688s 2s/step - loss: 0.3704 - sparse_categorical_accuracy: 0.8485 - val_loss: 2.3143 - val_sparse_categorical_accuracy: 0.4261\n",
      "Epoch 3/40\n",
      "1050/1050 [==============================] - 1773s 2s/step - loss: 0.3463 - sparse_categorical_accuracy: 0.8604 - val_loss: 2.3576 - val_sparse_categorical_accuracy: 0.4225\n",
      "Epoch 4/40\n",
      "1050/1050 [==============================] - 1897s 2s/step - loss: 0.3316 - sparse_categorical_accuracy: 0.8674 - val_loss: 2.3732 - val_sparse_categorical_accuracy: 0.4263\n",
      "Epoch 5/40\n",
      "1050/1050 [==============================] - 1856s 2s/step - loss: 0.3345 - sparse_categorical_accuracy: 0.8659 - val_loss: 5.6442 - val_sparse_categorical_accuracy: 0.3721\n",
      "Epoch 6/40\n",
      "1050/1050 [==============================] - 1828s 2s/step - loss: 0.3405 - sparse_categorical_accuracy: 0.8672 - val_loss: 61.0254 - val_sparse_categorical_accuracy: 0.0456\n",
      "Epoch 7/40\n",
      "1050/1050 [==============================] - 1658s 2s/step - loss: 0.3227 - sparse_categorical_accuracy: 0.8689 - val_loss: 2.8594 - val_sparse_categorical_accuracy: 0.4330\n",
      "Epoch 8/40\n",
      " 899/1050 [========================>.....] - ETA: 3:46 - loss: 0.3267 - sparse_categorical_accuracy: 0.8685"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def scheduler(epoch):\n",
    "  if epoch < 1:\n",
    "    return 0.001\n",
    "  else:\n",
    "    return 0.001 * tf.math.exp(0.1 * (1 - epoch))\n",
    "\n",
    "checkpoint_path = 'Percentage_'+ str(percentageData) + '/weights-{epoch:03d}.ckpt'\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_path, \n",
    "    verbose=1, \n",
    "    save_weights_only=True,\n",
    "    save_freq=len(train_dataframe) *QtdEpocasGravarCHKP)\n",
    "\n",
    "print(QtdEpocasGravarCHKP*lenghtDataTrain)\n",
    "\n",
    "#LRS = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "csv_callback=tf.keras.callbacks.CSVLogger(\n",
    "   'Percentage_'+ str(percentageData) + '/'+ \"Training Parameters.csv\",\n",
    "    separator=',', append=False\n",
    ")\n",
    "\n",
    "\n",
    "with tf.device(device) :\n",
    "    history =  model.fit(train_dataset,\n",
    "              validation_data=validation_dataset,\n",
    "              callbacks=[csv_callback,cp_callback],\n",
    "              epochs=Epochs)\n",
    "\n",
    "\n",
    "hist = pd.DataFrame(history.history)\n",
    "hist['epoch'] = history.epoch\n",
    "history.history\n",
    "hist.tail()\n",
    "#plot_loss(history)\n",
    "#plot_acc(history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843682ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = pd.DataFrame(history.history)\n",
    "hist['epoch'] = history.epoch\n",
    "history.history\n",
    "hist.tail()\n",
    "plot_loss(history)\n",
    "plot_acc(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cc7a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "contadorCerto =0\n",
    "contadorErro =0 \n",
    "ArrayQuestoesCertas =[]\n",
    "ArrarQuestoesErradas=[]\n",
    "\n",
    "for contador in range(len(train_dataframe)) :\n",
    "   \n",
    "    im=cv2.imread(train_dataframe.iloc[contador]['Path'])\n",
    "    im=cv2.resize(im,(200,200))\n",
    "    q=train_dataframe.iloc[contador]['Question'] \n",
    "    q=encoder.encode(q)\n",
    "    paddings = [[0, 50-tf.shape(q)[0]]]\n",
    "    q=tf.pad(q, paddings, 'CONSTANT', constant_values=0)\n",
    "    q=np.array(q)\n",
    "    im.resize(1,200,200,3)\n",
    "    q.resize(1,50)\n",
    "    ans=model.predict([im,q]) \n",
    "    decodAns = encoder.decode([np.argmax(ans)])\n",
    "        \n",
    "    \n",
    "    if train_dataframe.iloc[contador]['Answer'] != decodAns :\n",
    "        ArrarQuestoesErradas.append (\"qestão numero : \" + str(contador))\n",
    "        ArrarQuestoesErradas.append(train_dataframe.iloc[contador]['Question'])\n",
    "        ArrarQuestoesErradas.append('Repost errada: ' +decodAns + \" resp certa:\" + train_dataframe.iloc[contador]['Answer']) \n",
    "        contadorErro = contadorErro +1\n",
    "    else:\n",
    "        ArrayQuestoesCertas.append(\"qestão numero : \" + str(contador))\n",
    "        ArrayQuestoesCertas.append(train_dataframe.iloc[contador]['Question'])\n",
    "        ArrayQuestoesCertas.append(  decodAns)\n",
    "        ArrayQuestoesCertas.append(train_dataframe.iloc[contador]['Answer'])\n",
    "        contadorCerto = contadorCerto +1\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed20b932",
   "metadata": {},
   "outputs": [],
   "source": [
    "print( \"questoes certas\" + str(contadorCerto))\n",
    "print( \"questoes erradas\" + str(contadorErro))\n",
    "VerifTrainList=[]\n",
    "VerifTrainList.append(\"Acerto - \" + str(contadorCerto))\n",
    "VerifTrainList.append(\"Erro - \" + str(contadorErro ))\n",
    "GravarArquivo(VerifTrainList,'Verif_Train_resumo' )\n",
    "GravarArquivo(ArrayQuestoesCertas,'Verif_train_questoescertas' )\n",
    "GravarArquivo(ArrarQuestoesErradas,'Verif_train_questoeserradas' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ff607e",
   "metadata": {},
   "outputs": [],
   "source": [
    "contadorCerto =0\n",
    "contadorErro =0 \n",
    "ArrayQuestoesCertas =[]\n",
    "ArrarQuestoesErradas=[]\n",
    "\n",
    "for contador in range(len(val_dataframe)) :\n",
    "   \n",
    "    im=cv2.imread(val_dataframe.iloc[contador]['Path'])\n",
    "    im=cv2.resize(im,(200,200))\n",
    "    q=val_dataframe.iloc[contador]['Question'] \n",
    "    q=encoder.encode(q)\n",
    "    paddings = [[0, 50-tf.shape(q)[0]]]\n",
    "    q=tf.pad(q, paddings, 'CONSTANT', constant_values=0)\n",
    "    q=np.array(q)\n",
    "    im.resize(1,200,200,3)\n",
    "    q.resize(1,50)\n",
    "    ans=model.predict([im,q]) \n",
    "    decodAns = encoder.decode([np.argmax(ans)])\n",
    "    if val_dataframe.iloc[contador]['Answer'] != decodAns :\n",
    "        ArrarQuestoesErradas.append (\"qestão numero : \" + str(contador))\n",
    "        ArrarQuestoesErradas.append(val_dataframe.iloc[contador]['Question'])\n",
    "        ArrarQuestoesErradas.append( \" errado - \" + decodAns + \" certo - \" + val_dataframe.iloc[contador]['Answer'] )\n",
    "              \n",
    "        contadorErro = contadorErro +1\n",
    "    else:\n",
    "        ArrayQuestoesCertas.append (\"qestão numero : \" + str(contador))\n",
    "        ArrayQuestoesCertas.append(val_dataframe.iloc[contador]['Question'])\n",
    "        ArrayQuestoesCertas.append(decodAns)\n",
    "        ArrayQuestoesCertas.append(val_dataframe.iloc[contador]['Answer'])\n",
    "       \n",
    "        contadorCerto = contadorCerto +1\n",
    "      \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3efd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "VerifValList=[]\n",
    "print(\"certo : \" + str(contadorCerto))\n",
    "print(\"erro : \" + str(contadorErro))\n",
    "tempo = (time.clock() - start_time, \"segundos\")\n",
    "print(tempo)\n",
    "VerifValList.append(\"Acerto - \" + str(contadorCerto))\n",
    "\n",
    "VerifValList.append(\"Erro - \" + str(contadorErro ))\n",
    "VerifValList.append(\"tempo - \" + str(contadorErro ))\n",
    "GravarArquivo(VerifValList,'Verif_Val_resumo' )\n",
    "GravarArquivo(ArrayQuestoesCertas,'Verif_val_questoescertas' )\n",
    "GravarArquivo(ArrarQuestoesErradas,'Verif_val_questoeserradas' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db148833",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.models.save_model(model,'Percentage_'+ str(percentageData) +  '/ModelTreinamento_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96786bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('Percentage_'+ str(percentageData) +  '/2train_dataframe', \"wb\") as f:\n",
    "    pickle.dump(train_dataframe, f)\n",
    "with open('Percentage_'+ str(percentageData) +  '/2encoder', \"wb\") as f:\n",
    "    pickle.dump(encoder, f)\n",
    "\n",
    "with open('Percentage_'+ str(percentageData) +  '/2val_dataframe', \"wb\") as f:\n",
    "    pickle.dump(val_dataframe, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac7b22f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88613b9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47368aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f57b3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee0501e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
