{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37f435f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oi\n",
      "['Percentage_50', 'Percentage_100', 'Percentage_25', 'Percentage_75', 'Percentage_15', '.ipynb_checkpoints']\n",
      "['weights-improvement-02-0.99.ckpt.index', 'weights-improvement-01-0.99.ckpt.index', 'weights-improvement-01-0.99.ckpt.data-00000-of-00001', 'Untitled.ipynb', 'checkpoint', 'accuracy_datalenght149991.png', 'train_datalenght149991.json', 'vocab_set_149991.json', 'ques_datalenght149991.json', 'error_datalenght149991.png', 'weights-improvement-02-0.99.ckpt.data-00000-of-00001']\n",
      "train\n",
      "train -fim\n",
      "fim - vocab\n",
      "qes - inicio\n",
      "qes - fim\n",
      "Testing the Encoder with sample questions - \n",
      " \n",
      "Original Text = How many other things are there of the same shape as the tiny cyan matte object?\n",
      "After Encoding = [89, 69, 95, 47, 58, 74, 24, 31, 52, 59, 48, 31, 21, 44, 99, 12]\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "text_input (InputLayer)         [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 50, 256)      25856       text_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "image_input (InputLayer)        [(None, 200, 200, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   (None, 50, 512)      1050624     embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mobilenetv2_1.00_224 (Functiona (None, 7, 7, 1280)   2257984     image_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 50, 512)      1574912     bidirectional[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d (Globa (None, 1280)         0           mobilenetv2_1.00_224[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 1024)         4198400     bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 2304)         0           global_average_pooling2d[0][0]   \n",
      "                                                                 bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 101)          232805      concatenate[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 9,340,581\n",
      "Trainable params: 9,306,469\n",
      "Non-trainable params: 34,112\n",
      "__________________________________________________________________________________________________\n",
      "/home/jupyter/imported///Percentage_100//weights-improvement-02-0.99.ckpt\n",
      " ************ carregou *************\n",
      "gravar arquivo2\n",
      "grupo1\n",
      "gravar arquivo2\n",
      "grupo2\n",
      "gravar arquivo2\n",
      "grupo3\n",
      "gravar arquivo2\n",
      "grupo4\n",
      "gravar arquivo2\n",
      "grupo5\n",
      "gravar arquivo2\n",
      "grupo6\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd   \n",
    "import os\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import json\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import os\n",
    "import nltk\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from keras.preprocessing import text\n",
    "import os\n",
    "import nltk\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "grupo = 0\n",
    "grupo = grupo + 1\n",
    "\n",
    "\n",
    "from os.path import isfile, join\n",
    "\n",
    "def GravarArquivo ( data_dict,fname):\n",
    "    print(\"gravar arquivo\" + str(len(data)))\n",
    "    if os.path.isfile(fname):\n",
    "    # File exists\n",
    "     with open(fname, 'a+') as outfile:\n",
    "        outfile.seek(outfile.tell() - 2, os.SEEK_SET)\n",
    "        outfile.truncate()\n",
    "        outfile.write(',')\n",
    "        outfile.write(json.dumps(data_dict,ensure_ascii=False, indent=4)[1:-1])\n",
    "        outfile.write(']')\n",
    "        outfile.close() \n",
    "    else: \n",
    "    # Create file\n",
    "     with open(fname, 'w') as outfile:\n",
    "        json.dump(data_dict, outfile, ensure_ascii=False, indent=4) \n",
    "        outfile.close()\n",
    "\n",
    "print('oi')\n",
    "file_to_search = '/home/jupyter/imported/'\n",
    " FolderSource = file_to_search + '//' + folder \n",
    "folders = dirs = [d for d in os.listdir(file_to_search) if os.path.isdir(os.path.join(file_to_search, d))]\n",
    "print(folders)\n",
    "for folder in folders:\n",
    "    if 'Percentage_4'  in folder:\n",
    "        \n",
    "        qtd = 0 \n",
    "     \n",
    "        vocab_set=set()#set object used to store the vocabulary\n",
    "        partSplit = folder.split('_')\n",
    "        perc = partSplit[1]\n",
    "        FolderSource = file_to_search + '//' + folder \n",
    "        Files = [f for f in os.listdir(FolderSource) if isfile(join(FolderSource, f))]\n",
    "        posQues =[]\n",
    "        valList=[]\n",
    "        trainList=[]\n",
    "        memorys = []\n",
    "        print(Files)\n",
    "        for File in Files:\n",
    "\n",
    "                if 'vocab' in File : \n",
    "                    \n",
    "                    with open(FolderSource + '//' + File) as f:\n",
    "\n",
    "                         vocabs = json.load(f)\n",
    "                         for vocab in vocabs :\n",
    "                            \n",
    "                            vocab_set.add(vocab)\n",
    "                    print('fim - vocab')\n",
    "\n",
    "                elif 'ques' in File:     \n",
    "                    with open('/home/kaggle/input/clevr-dataset/CLEVR_v1.0/questions/CLEVR_val_questions.json') as que:\n",
    "                        print(\"qes - inicio\")\n",
    "                        data = json.load(que)\n",
    "                        with open(FolderSource + '//' + File) as f:\n",
    "                             ques = json.load(f)\n",
    "                             for que in ques :\n",
    "                                pos = que    \n",
    "                                i = data['questions'][pos]\n",
    "                                temp=[]\n",
    "                                \n",
    "                                for path in glob.glob('/home/kaggle/input/clevr-dataset/CLEVR_v1.0/images/val/'+i['image_filename']): \n",
    "                                    temp.append(path)\n",
    "                                temp.append(i['question'])\n",
    "                                temp.append(i['answer'])\n",
    "                                temp.append(pos)\n",
    "                                valList.append(temp)\n",
    "                        f.close()\n",
    "                        print(\"qes - fim\")\n",
    "\n",
    "                elif 'train' in File:\n",
    "                    print('train')\n",
    "                    temp=[]    \n",
    "                    with open('/home/kaggle/input/clevr-dataset/CLEVR_v1.0/questions/CLEVR_train_questions.json') as train:\n",
    "                        data = json.load(train)\n",
    "                       \n",
    "                        with open(FolderSource + '//' + File) as f:\n",
    "                            Ftrain = json.load(f)\n",
    "                            for tr in Ftrain:\n",
    "                                pos = tr\n",
    "                              \n",
    "                                i = data['questions'][pos]\n",
    "                              \n",
    "                                temp=[]\n",
    "                                for path in glob.glob('/home/kaggle/input/clevr-dataset/CLEVR_v1.0/images/train/'+i['image_filename']): \n",
    "                                    temp.append(path)\n",
    "                                temp.append(i['question'])\n",
    "                                temp.append(i['answer'])\n",
    "                              \n",
    "                                trainList.append(temp)\n",
    "                        f.close()\n",
    "                        \n",
    "                    print('train -fim')\n",
    "                    train.close()\n",
    "                elif 'ckpt' in File and 'index' in File:\n",
    "                    obj=[]\n",
    "                    obj.append(perc)\n",
    "                    obj.append(File[:-6])\n",
    "                    memorys.append(obj)\n",
    "       \n",
    "        encoder=tfds.features.text.TokenTextEncoder(vocab_set)\n",
    "        print(\"Testing the Encoder with sample questions - \\n \")\n",
    "        example_text=encoder.encode(trainList[1][1])\n",
    "        print(\"Original Text = \"+trainList[1][1])\n",
    "\n",
    "        print(\"After Encoding = \"+str(example_text))\n",
    "        Traninlist=[]\n",
    "       \n",
    "        for memory in memorys :\n",
    "           if 'weights-improvement-05' in memory\n",
    "                CNN_Input=tf.keras.layers.Input(shape=(200,200,3),name='image_input')\n",
    "\n",
    "                mobilenetv2=tf.keras.applications.mobilenet_v2.MobileNetV2(input_shape=(200,200,3), alpha=1.0, include_top=False,\n",
    "                                                                weights='imagenet', input_tensor=CNN_Input)\n",
    "\n",
    "                CNN_model=tf.keras.models.Sequential()\n",
    "                CNN_model.add(CNN_Input)\n",
    "                CNN_model.add(mobilenetv2)\n",
    "                CNN_model.add(tf.keras.layers.GlobalAveragePooling2D())\n",
    "\n",
    "                #Creating the RNN model for text processing\n",
    "                RNN_model=tf.keras.models.Sequential()\n",
    "                RNN_Input=tf.keras.layers.Input(shape=(50),name='text_input')\n",
    "                RNN_model.add(RNN_Input)\n",
    "                RNN_model.add(tf.keras.layers.Embedding (len(vocab_set)+1,256))\n",
    "                RNN_model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256,stateful=False,return_sequences=True,recurrent_initializer='glorot_uniform')))\n",
    "                RNN_model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256,stateful=False,return_sequences=True,recurrent_initializer='glorot_uniform')))\n",
    "                RNN_model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(512,stateful=False,return_sequences=False,recurrent_initializer='glorot_uniform')))\n",
    "\n",
    "\n",
    "                concat=tf.keras.layers.concatenate([CNN_model.output,RNN_model.output])\n",
    "                dense_out=tf.keras.layers.Dense(len(vocab_set)+1,activation='softmax',name='output')(concat)\n",
    "\n",
    "                model = tf.keras.Model(inputs=[CNN_Input,RNN_Input],\n",
    "                                outputs=dense_out)\n",
    "                model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',\n",
    "                        metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
    "                model.summary()\n",
    "\n",
    "                filepath=FolderSource + '//' + memory[1]  \n",
    "                print(filepath)\n",
    "                model.load_weights(filepath)\n",
    "                print(' ************ carregou *************')\n",
    "                #generate dates to test \n",
    "\n",
    "                index=1\n",
    "                #0 - path image \n",
    "                #1 - address \n",
    "                ListasAcertasTrainlist = []\n",
    "                for contador in range(len(trainList)) :\n",
    "\n",
    "                    im=cv2.imread(trainList[contador][0])\n",
    "                    im=cv2.resize(im,(200,200))\n",
    "                    q=trainList[contador][1]  \n",
    "                    q=encoder.encode(q)\n",
    "                    paddings = [[0, 50-tf.shape(q)[0]]]\n",
    "                    q=tf.pad(q, paddings, 'CONSTANT', constant_values=0)\n",
    "                    q=np.array(q)\n",
    "                    im.resize(1,200,200,3)\n",
    "                    q.resize(1,50)\n",
    "                    ans=model.predict([im,q]) \n",
    "                    decodAns = encoder.decode([np.argmax(ans)])\n",
    "                    if trainList[contador][2] != decodAns :\n",
    "                        ListasAcertasTrainlist.append(0)\n",
    "\n",
    "                    else:     \n",
    "                        ListasAcertasTrainlist.append(1)\n",
    "                    if (contador%2000) == 0 : \n",
    "                        GravarArquivo (ListasAcertasTrainlist,FolderSource + '/ResultPredication_Train_' +memory[1][:-5] +'.json')   \n",
    "                        print(\"grupo\" + str(grupo ) )\n",
    "                        grupo = grupo + 1\n",
    "                        ListasAcertasTrainlist=[]\n",
    "                if (contador%2000) >= 0 :\n",
    "                    GravarArquivo (ListasAcertasTrainlist,FolderSource + '/ResultPredication_Train_' +memory[1][:-5] +'.json')   \n",
    "                ListasValidacaolist = []\n",
    "                for contador in   range(len(valList)) :\n",
    "\n",
    "                    im=cv2.imread(valList[contador][0])\n",
    "                    im=cv2.resize(im,(200,200))\n",
    "                    q=valList[contador][1]\n",
    "                    q=encoder.encode(q)\n",
    "                    paddings = [[0, 50-tf.shape(q)[0]]]\n",
    "                    q=tf.pad(q, paddings, 'CONSTANT', constant_values=0)\n",
    "                    q=np.array(q)\n",
    "                    im.resize(1,200,200,3)\n",
    "                    q.resize(1,50)\n",
    "\n",
    "                    ans=model.predict([im,q]) \n",
    "                    decodAns = encoder.decode([np.argmax(ans)])\n",
    "                    if valList[contador][2] != decodAns :\n",
    "                        ListasValidacaolist.append(0)\n",
    "\n",
    "                    else:     \n",
    "                        ListasValidacaolist.append(1)\n",
    "                    if (contador%2000) == 0 : \n",
    "                        GravarArquivo (ListasValidacaolist,FolderSource + '/ResultPredication_Val_' +memory[1][:-5] +'.json')   \n",
    "                      \n",
    "                        grupo = grupo +1\n",
    "                        ListasValidacaolist=[]     \n",
    "                if (contador%2000) >= 0 :      \n",
    "                     GravarArquivo (ListasValidacaolist,FolderSource + '/ResultPredication_Val_' +memory[1][:-5] +'.json')   \n",
    "\n",
    "    \n",
    "    \n",
    "print('fim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f78f512",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
