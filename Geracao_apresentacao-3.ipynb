{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6be1298a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fim\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd   \n",
    "import os\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import json\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import os\n",
    "import nltk\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "\n",
    "print('fim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec3f9bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "from keras.backend import manual_variable_initialization\n",
    "manual_variable_initialization(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b13285f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GravarArquivo ( data_dict,fname):\n",
    "    fname = fname +\"_\" +str(len(data_dict)) + '.json'\n",
    "    print(\"gravar arquivo: \" + fname + \" qtd: \" +  str(len(data_dict)))\n",
    "    os.makedirs('Percentage_'+ str(percentageData) , exist_ok=True)\n",
    "    fname = 'Percentage_'+ str(percentageData) + \"/\" + fname\n",
    "    # Create file\n",
    "    with open(fname, 'w') as outfile:\n",
    "        json.dump(data_dict, outfile, ensure_ascii=False, indent=4) \n",
    "        outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c1ef94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(history):\n",
    "            plt.plot(history.history['loss'], label='training loss')\n",
    "            plt.plot(history.history['val_loss'], label=' validation loss')\n",
    "            #plt.ylim([0, 10])\n",
    "            plt.xlabel('Epopch')\n",
    "            plt.ylabel('Error')\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.savefig( 'Percentage_'+ str(percentageData) + '/error_datalenght.png')\n",
    "            plt.show()\n",
    "\n",
    "def plot_acc(history):  \n",
    "            plt.plot(history.history['sparse_categorical_accuracy'])\n",
    "            plt.plot(history.history['val_sparse_categorical_accuracy'])\n",
    "            plt.title('model accuracy')\n",
    "            plt.ylabel('accuracy')\n",
    "            plt.xlabel('epoch')\n",
    "            plt.legend(['train', 'val'], loc='upper left')\n",
    "            plt.grid(True)\n",
    "            plt.savefig('Percentage_'+ str(percentageData) + '/accuracy_datalenght.png')\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3863552c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs : 0\n",
      "Tensorflow GPU : True\n"
     ]
    }
   ],
   "source": [
    "#Check GPU is available for training or not Or whether the tensorflow version can utilize gpu \n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "print(\"Number of GPUs :\", len(physical_devices)) \n",
    "print(\"Tensorflow GPU :\",tf.test.is_built_with_cuda())\n",
    "if len(physical_devices)>0:\n",
    "    device=\"/GPU:0\"\n",
    "else:\n",
    "    device=\"/CPU:0\"\n",
    "percentageData = 3\n",
    "posTrainList=[]\n",
    "posValList=[]\n",
    "BATCH_SIZE=1\n",
    "IMG_SIZE=(200,200)\n",
    "QtdEpocasGravarCHKP = 10\n",
    "Epochs = 60\n",
    "lenghtDataTrain = 0\n",
    "lenghtDataVal =0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be03030c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oi\n"
     ]
    }
   ],
   "source": [
    "trainList=[]\n",
    "\n",
    "with open('/home/jupyter/input/clevr-dataset/CLEVR_v1.0/questions/CLEVR_train_questions.json') as f:\n",
    "    print('oi')\n",
    "    data = json.load(f)\n",
    "    lenghtDataTrain = int(len(data['questions']) * (percentageData/100))\n",
    "    print(lenghtDataTrain)\n",
    "    for K in range(lenghtDataTrain):\n",
    "        i = data['questions'][K]\n",
    "       \n",
    "        temp=[]\n",
    "        for path in glob.glob('/home/jupyter/input/clevr-dataset/CLEVR_v1.0/images/train/'+i['image_filename']): \n",
    "            temp.append(path)\n",
    "           \n",
    "        temp.append(i['question'])\n",
    "        temp.append(i['answer'])\n",
    "        trainList.append(temp)\n",
    "        posTrainList.append(K)\n",
    "f.close()\n",
    "labels=['Path','Question','Answer']\n",
    "train_dataframe = pd.DataFrame.from_records(trainList, columns=labels)#training Dataframe \n",
    "del(data)\n",
    "del(trainList)\n",
    "print('fim')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f62e558",
   "metadata": {},
   "outputs": [],
   "source": [
    "valList=[]\n",
    "with open('/home/jupyter/input/clevr-dataset/CLEVR_v1.0/questions/CLEVR_val_questions.json') as f:\n",
    "    data = json.load(f)\n",
    "    lenghtDataVal = int(len(data['questions']) * (percentageData/100))\n",
    "    for K in range(lenghtDataVal):\n",
    "    \n",
    "       \n",
    "        i = data['questions'][K]\n",
    "        \n",
    "        temp=[]\n",
    "        for path in glob.glob('/home/jupyter/input/clevr-dataset/CLEVR_v1.0/images/val/'+i['image_filename']): \n",
    "            temp.append(path)\n",
    "        temp.append(i['question'])\n",
    "        temp.append(i['answer'])\n",
    "        valList.append(temp)\n",
    "        posValList.append(K)\n",
    "f.close() \n",
    "\n",
    "val_dataframe = pd.DataFrame.from_records(valList, columns=labels)#validation Dataframe\n",
    "del(data)\n",
    "del(valList)\n",
    "val_dataframe.head()\n",
    "print('passou carregou')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2312898a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb7f7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_set=set()#set object used to store the vocabulary\n",
    "\n",
    "tokenizer = tfds.features.text.Tokenizer()\n",
    "valtest=[]\n",
    "\n",
    "valList =[]\n",
    "with open('/home/jupyter/input/clevr-dataset/CLEVR_v1.0/questions/CLEVR_train_questions.json') as f:\n",
    "    print('oi')\n",
    "    data = json.load(f)\n",
    "   \n",
    "    print(int(len(data['questions'])))\n",
    "    for K in range(int(len(data['questions']))):\n",
    "        i = data['questions'][K]\n",
    "        temp=[]\n",
    "        for path in glob.glob('/home/jupyter/input/clevr-dataset/CLEVR_v1.0/images/train/'+i['image_filename']): \n",
    "            temp.append(path)\n",
    "        temp.append(i['question'])\n",
    "        temp.append(i['answer'])\n",
    "        valtest.append(temp)\n",
    "   \n",
    "f.close()\n",
    "labels=['Path','Question','Answer']\n",
    "train_dataframe2 = pd.DataFrame.from_records(valtest, columns=labels)#training Dataframe \n",
    "del(data)\n",
    "del(valtest)\n",
    "print('fim')\n",
    "\n",
    "valList=[]\n",
    "with open('/home/jupyter/input/clevr-dataset/CLEVR_v1.0/questions/CLEVR_val_questions.json') as f:\n",
    "    data = json.load(f)\n",
    "    lenghtDataVal = int(len(data['questions']) * (percentageData/100))\n",
    "    for K in range(int(len(data['questions']))):\n",
    "        i = data['questions'][K]\n",
    "        temp=[]\n",
    "        for path in glob.glob('/home/jupyter/input/clevr-dataset/CLEVR_v1.0/images/val/'+i['image_filename']): \n",
    "            temp.append(path)\n",
    "        temp.append(i['question'])\n",
    "        temp.append(i['answer'])\n",
    "        valList.append(temp)\n",
    "        \n",
    "f.close() \n",
    "\n",
    "val_dataframe2 = pd.DataFrame.from_records(valList, columns=labels)#validation Dataframe\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i in val_dataframe2['Question']:\n",
    "    vocab_set.update(tokenizer.tokenize(i))\n",
    "for i in train_dataframe2['Question']:\n",
    "    vocab_set.update(tokenizer.tokenize(i))\n",
    "for i in val_dataframe2['Answer']:\n",
    "    vocab_set.update(tokenizer.tokenize(i))\n",
    "for i in train_dataframe2['Answer']:\n",
    "    vocab_set.update(tokenizer.tokenize(i))\n",
    "    \n",
    "vocab_set.update('12aaaa')\n",
    "vocab_set.update('1234sssa')\n",
    "\n",
    "#\n",
    "#Creating an Encoder and a Function to preprocess the text data during the training and inference    \n",
    "    \n",
    "encoder=tfds.features.text.TokenTextEncoder(vocab_set)\n",
    "index=2\n",
    "print(\"Testing the Encoder with sample questions - \\n \")\n",
    "example_text=encoder.encode(train_dataframe['Question'][index])\n",
    "print(\"Original Text = \"+train_dataframe['Question'][index])\n",
    "print(\"After Encoding = \"+str(example_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c428e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(vocab_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9f56f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "GravarArquivo(posValList,'Val_pos' )\n",
    "GravarArquivo(posTrainList,'Train_pos' )\n",
    "GravarArquivo(list(vocab_set), 'Vocab_set' )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2588b02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Function that uses the encoder created to encode the input question and answer string\n",
    "def encode_fn(text):\n",
    "    return np.array(encoder.encode(text.numpy()))\n",
    "\n",
    "\n",
    "#Function to load and decode the image from the file paths in the dataframe and use the encoder function\n",
    "def preprocess(ip,ans):\n",
    "    img,ques=ip#ip is a list containing image paths and questions\n",
    "    img=tf.io.read_file(img)\n",
    "    img=tf.image.decode_jpeg(img,channels=3)\n",
    "    # quantos canais de cores tem \n",
    "    img=tf.image.resize(img,IMG_SIZE)\n",
    "    img=tf.math.divide(img, 255)# \n",
    "    #The question string is converted to encoded list with fixed size of 50 with padding with 0 value\n",
    "    ques=tf.py_function(encode_fn,inp=[ques],Tout=tf.int32)\n",
    "    paddings = [[0, 50-tf.shape(ques)[0]]]\n",
    "    ques = tf.pad(ques, paddings, 'CONSTANT', constant_values=0)\n",
    "    ques.set_shape([50])#Explicit shape must be defined in order to create the Input pipeline\n",
    "    \n",
    "    #The Answer is also encoded \n",
    "    ans=tf.py_function(encode_fn,inp=[ans],Tout=tf.int32)\n",
    "    ans.set_shape([1])\n",
    "    print(ans)\n",
    "    return (img,ques),ans\n",
    "    \n",
    "def create_pipeline(dataframe):\n",
    "    raw_df=tf.data.Dataset.from_tensor_slices(((dataframe['Path'],dataframe['Question']),dataframe['Answer']))\n",
    "    df=raw_df.map(preprocess)#Preprocessing function is applied to the dataset\n",
    "    df=df.batch(BATCH_SIZE)#The dataset is batched\n",
    "    return df\n",
    "\n",
    "#The training and validation Dataset objects are created\n",
    "train_dataset=create_pipeline(train_dataframe)\n",
    "validation_dataset=create_pipeline(val_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd23f523",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the CNN model for image processing\n",
    "\n",
    "\n",
    "CNN_Input=tf.keras.layers.Input(shape=(200,200,3),name='image_input')\n",
    "\n",
    "mobilenetv2=tf.keras.applications.mobilenet_v2.MobileNetV2(input_shape=(200,200,3), alpha=1.0, include_top=False,\n",
    "                                                      weights='imagenet', input_tensor=CNN_Input)\n",
    "\n",
    "CNN_model=tf.keras.models.Sequential()\n",
    "CNN_model.add(CNN_Input)\n",
    "CNN_model.add(mobilenetv2)\n",
    "CNN_model.add(tf.keras.layers.GlobalAveragePooling2D())\n",
    "\n",
    "\n",
    "\n",
    "#Creating the RNN model for text processing\n",
    "RNN_model=tf.keras.models.Sequential()\n",
    "\n",
    "RNN_Input=tf.keras.layers.Input(shape=(50),name='text_input')\n",
    "RNN_model.add(RNN_Input)\n",
    "RNN_model.add(tf.keras.layers.Embedding (len(vocab_set)+1,256))\n",
    "RNN_model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256,return_sequences=True,recurrent_initializer='glorot_uniform')))\n",
    "RNN_model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256,return_sequences=True,recurrent_initializer='glorot_uniform')))\n",
    "RNN_model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(512,return_sequences=False,recurrent_initializer='glorot_uniform')))\n",
    "\n",
    "\n",
    "concat=tf.keras.layers.concatenate([CNN_model.output,RNN_model.output])\n",
    "dense_out=tf.keras.layers.Dense(len(vocab_set)+1,activation='softmax',name='output')(concat)\n",
    "\n",
    "model = tf.keras.Model(inputs=[CNN_Input,RNN_Input],\n",
    "                    outputs=dense_out)\n",
    "model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',\n",
    "              metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
    "\n",
    "\n",
    "model.summary()\n",
    "tf.keras.utils.plot_model(model, show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40d6d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(QtdEpocasGravarCHKP*lenghtDataTrain)\n",
    "print(QtdEpocasGravarCHKP)\n",
    "print(lenghtDataTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee79790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20999/20999 [==============================] - 9667s 460ms/step - loss: 1.1033 - sparse_categorical_accuracy: 0.4225 - val_loss: 1.2026 - val_sparse_categorical_accuracy: 0.4263\n",
      "Epoch 5/60\n",
      " 1833/20999 [=>............................] - ETA: 2:18:44 - loss: 1.1298 - sparse_categorical_accuracy: 0.4452"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def scheduler(epoch):\n",
    "  if epoch < 1:\n",
    "    return 0.001\n",
    "  else:\n",
    "    return 0.001 * tf.math.exp(0.1 * (1 - epoch))\n",
    "\n",
    "checkpoint_path = 'Percentage_'+ str(percentageData) + '/weights-{epoch:03d}.ckpt'\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_path, \n",
    "    verbose=1, \n",
    "    save_weights_only=True,\n",
    "    save_freq=QtdEpocasGravarCHKP*lenghtDataTrain)\n",
    "\n",
    "print(QtdEpocasGravarCHKP*lenghtDataTrain)\n",
    "\n",
    "#LRS = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "csv_callback=tf.keras.callbacks.CSVLogger(\n",
    "   'Percentage_'+ str(percentageData) + '/'+ \"Training Parameters.csv\",\n",
    "    separator=',', append=False\n",
    ")\n",
    "\n",
    "\n",
    "with tf.device(device) :\n",
    "    history =  model.fit(train_dataset,\n",
    "              validation_data=validation_dataset,\n",
    "              callbacks=[csv_callback,cp_callback],\n",
    "              epochs=Epochs)\n",
    "\n",
    "\n",
    "hist = pd.DataFrame(history.history)\n",
    "hist['epoch'] = history.epoch\n",
    "history.history\n",
    "hist.tail()\n",
    "#plot_loss(history)\n",
    "#plot_acc(history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50f91c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = pd.DataFrame(history.history)\n",
    "hist['epoch'] = history.epoch\n",
    "history.history\n",
    "hist.tail()\n",
    "plot_loss(history)\n",
    "plot_acc(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea60899",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (' *************** vocab se *************************')\n",
    "for voc in vocab_set:\n",
    " print(voc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9be5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "contadorCerto =0\n",
    "contadorErro =0 \n",
    "ArrayQuestoesCertas =[]\n",
    "ArrarQuestoesErradas=[]\n",
    "\n",
    "for contador in range(lenghtDataTrain) :\n",
    "   \n",
    "    im=cv2.imread(train_dataframe.iloc[contador]['Path'])\n",
    "    im=cv2.resize(im,(200,200))\n",
    "    q=train_dataframe.iloc[contador]['Question'] \n",
    "    q=encoder.encode(q)\n",
    "    paddings = [[0, 50-tf.shape(q)[0]]]\n",
    "    q=tf.pad(q, paddings, 'CONSTANT', constant_values=0)\n",
    "    q=np.array(q)\n",
    "    im.resize(1,200,200,3)\n",
    "    q.resize(1,50)\n",
    "    ans=model.predict([im,q]) \n",
    "    decodAns = encoder.decode([np.argmax(ans)])\n",
    "        \n",
    "    \n",
    "    if train_dataframe.iloc[contador]['Answer'] != decodAns :\n",
    "        ArrarQuestoesErradas.append (\"qest達o numero : \" + str(contador))\n",
    "        ArrarQuestoesErradas.append(train_dataframe.iloc[contador]['Question'])\n",
    "        ArrarQuestoesErradas.append('Repost errada: ' +decodAns + \" resp certa:\" + train_dataframe.iloc[contador]['Answer']) \n",
    "        contadorErro = contadorErro +1\n",
    "    else:\n",
    "        ArrayQuestoesCertas.append(\"qest達o numero : \" + str(contador))\n",
    "        ArrayQuestoesCertas.append(train_dataframe.iloc[contador]['Question'])\n",
    "        ArrayQuestoesCertas.append(  decodAns)\n",
    "        ArrayQuestoesCertas.append(train_dataframe.iloc[contador]['Answer'])\n",
    "        contadorCerto = contadorCerto +1\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446d0081",
   "metadata": {},
   "outputs": [],
   "source": [
    "print( \"questoes certas\" + str(contadorCerto))\n",
    "print( \"questoes erradas\" + str(contadorErro))\n",
    "VerifTrainList=[]\n",
    "VerifTrainList.append(\"Acerto - \" + str(contadorCerto))\n",
    "VerifTrainList.append(\"Erro - \" + str(contadorErro ))\n",
    "GravarArquivo(VerifTrainList,'Verif_Train_resumo' )\n",
    "GravarArquivo(ArrayQuestoesCertas,'Verif_train_questoescertas' )\n",
    "GravarArquivo(ArrarQuestoesErradas,'Verif_train_questoeserradas' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95bac4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "contadorCerto =0\n",
    "contadorErro =0 \n",
    "ArrayQuestoesCertas =[]\n",
    "ArrarQuestoesErradas=[]\n",
    "\n",
    "for contador in range(lenghtDataVal) :\n",
    "   \n",
    "    im=cv2.imread(val_dataframe.iloc[contador]['Path'])\n",
    "    im=cv2.resize(im,(200,200))\n",
    "    q=val_dataframe.iloc[contador]['Question'] \n",
    "    q=encoder.encode(q)\n",
    "    paddings = [[0, 50-tf.shape(q)[0]]]\n",
    "    q=tf.pad(q, paddings, 'CONSTANT', constant_values=0)\n",
    "    q=np.array(q)\n",
    "    im.resize(1,200,200,3)\n",
    "    q.resize(1,50)\n",
    "    ans=model.predict([im,q]) \n",
    "    decodAns = encoder.decode([np.argmax(ans)])\n",
    "    if val_dataframe.iloc[contador]['Answer'] != decodAns :\n",
    "        ArrarQuestoesErradas.append (\"qest達o numero : \" + str(contador))\n",
    "        ArrarQuestoesErradas.append(val_dataframe.iloc[contador]['Question'])\n",
    "        ArrarQuestoesErradas.append( \" errado - \" + decodAns + \" certo - \" + val_dataframe.iloc[contador]['Answer'] )\n",
    "              \n",
    "        contadorErro = contadorErro +1\n",
    "    else:\n",
    "        ArrayQuestoesCertas.append (\"qest達o numero : \" + str(contador))\n",
    "        ArrayQuestoesCertas.append(val_dataframe.iloc[contador]['Question'])\n",
    "        ArrayQuestoesCertas.append(decodAns)\n",
    "        ArrayQuestoesCertas.append(val_dataframe.iloc[contador]['Answer'])\n",
    "       \n",
    "        contadorCerto = contadorCerto +1\n",
    "      \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cee244",
   "metadata": {},
   "outputs": [],
   "source": [
    "VerifValList=[]\n",
    "print(\"certo : \" + str(contadorCerto))\n",
    "print(\"erro : \" + str(contadorErro))\n",
    "VerifValList.append(\"Acerto - \" + str(contadorCerto))\n",
    "VerifValList.append(\"Erro - \" + str(contadorErro ))\n",
    "GravarArquivo(VerifValList,'Verif_Val_resumo' )\n",
    "GravarArquivo(ArrayQuestoesCertas,'Verif_val_questoescertas' )\n",
    "GravarArquivo(ArrarQuestoesErradas,'Verif_val_questoeserradas' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81543b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.models.save_model(model,'Percentage_'+ str(percentageData) +  '/ModelTreinamento')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ab67e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('Percentage_'+ str(percentageData) +  '/train_dataframe', \"wb\") as f:\n",
    "    pickle.dump(train_dataframe, f)\n",
    "with open('Percentage_'+ str(percentageData) +  '/encoder', \"wb\") as f:\n",
    "    pickle.dump(encoder, f)\n",
    "\n",
    "with open('Percentage_'+ str(percentageData) +  '/val_dataframe', \"wb\") as f:\n",
    "    pickle.dump(val_dataframe, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bee5bb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b4414f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
